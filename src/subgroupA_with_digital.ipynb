{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"../data/BankChurners.csv\")\n",
    "balance_df = pd.read_csv(\"../data/botswana_bank_customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing irrelevent columns\n",
    "original = original.drop(original.columns[[-1, -2]], axis=1)\n",
    "original = original.drop(columns=['Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1','Contacts_Count_12_mon','Total_Ct_Chng_Q4_Q1'])\n",
    "\n",
    "# renaming the datasets\n",
    "original = original.rename(columns={'Months_on_book' : 'Month_with_bank',\n",
    "                                    'Total_Relationship_Count' : 'No_of_product',\n",
    "                                    'Total_Trans_Ct' : 'Total_Trans_Count'})\n",
    "\n",
    "# removing Na from the dataset\n",
    "original_Unknown = original[original.isin(['Unknown']).any(axis=1)] # someone handle the unknown please\n",
    "original = original[~original.isin(['Unknown']).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will remove the k,$ and + sign in the income category column\n",
    "def clean_col(x):\n",
    "        if 'K' in x:\n",
    "            return x.replace('K','').replace('$','')\n",
    "        elif '+' in x:\n",
    "            return x.replace('+','')\n",
    "        elif x =='Less than 40':\n",
    "            return x.split()[2]\n",
    "        return x\n",
    "    \n",
    "original['Income_Category']=original['Income_Category'].apply(clean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object into category\n",
    "categorical_features = ['Attrition_Flag','Gender','Education_Level','Marital_Status','Income_Category','Card_Category']\n",
    "for category in categorical_features:\n",
    "    original[category] = original[category].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting from the original data\n",
    "required_features = [\"CLIENTNUM\",\"Customer_Age\",\"Gender\",\"Income_Category\",\"No_of_product\"]\n",
    "subset_original = original.loc[:,required_features]\n",
    "\n",
    "# subsetting from the income data\n",
    "required_features2 = ['Date of Birth','Gender','Income','NumOfProducts']\n",
    "subset_balance = balance_df.loc[:,required_features2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date of birth to date time and them convert it to age\n",
    "subset_balance['Date of Birth'] = pd.to_datetime(subset_balance['Date of Birth'])\n",
    "reference_date = pd.Timestamp('2024-01-01')\n",
    "subset_balance['Date of Birth'] = reference_date.year - subset_balance['Date of Birth'].dt.year\n",
    "\n",
    "# Changing income into income category\n",
    "bins = [0, 40000, 60000, 80000, 120000, float('inf')]\n",
    "labels = ['Less than 40', '40 - 60', '60 - 80', '80 - 120', '120 +']\n",
    "subset_balance['Income'] = pd.cut(subset_balance['Income'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Recoding Male to M...\n",
    "subset_balance['Gender'] = subset_balance['Gender'].replace({'Male':'M','Female':'F'})\n",
    "\n",
    "# Renaming the balance_subset dataframe\n",
    "subset_balance = subset_balance.rename(columns = {'Date of Birth' : \"Customer_Age\", \n",
    "                                        'Income' : \"Income_Category\", 'NumOfProducts':\"No_of_product\"})\n",
    "\n",
    "# Converting object to category\n",
    "subset_balance['Gender'] = subset_balance['Gender'].astype('category')\n",
    "\n",
    "# Scaling \n",
    "scaler = MinMaxScaler()\n",
    "subset_balance[['Customer_Age','No_of_product']] = scaler.fit_transform(subset_balance[['Customer_Age','No_of_product']])\n",
    "subset_original[['Customer_Age','No_of_product']] = scaler.fit_transform(subset_original[['Customer_Age','No_of_product']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savings and Investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Savings\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "original_copy = original\n",
    "sav = pd.read_csv(\"../data/credit_score.csv\") \n",
    "# https://www.kaggle.com/datasets/conorsully1/credit-score?resource=download\n",
    "\n",
    "# convert income into income category then one-hot encoding\n",
    "bins = [0, 40000, 60000, 80000, 120000, float('inf')]\n",
    "labels = ['Less than 40', '40 - 60', '60 - 80', '80 - 120', '120 +']\n",
    "sav['Income_Category'] = pd.cut(sav['INCOME'], bins = bins, labels = labels, right = False)\n",
    "sav = pd.get_dummies(sav, columns=['Income_Category'], drop_first=True)\n",
    "\n",
    "# clustering\n",
    "features = sav[['Income_Category_40 - 60', 'Income_Category_60 - 80', 'Income_Category_80 - 120', 'Income_Category_120 +']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "sav['Cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# fitting normal distribution\n",
    "stats = sav.groupby('Cluster')['SAVINGS'].agg(['mean', 'std']).reset_index()\n",
    "stats.columns = ['Cluster', 'Mean', 'SD']\n",
    "\n",
    "def sample(cluster):\n",
    "    mean = stats.loc[stats['Cluster'] == cluster, 'Mean'].values[0]\n",
    "    sd = stats.loc[stats['Cluster'] == cluster, 'SD'].values[0]\n",
    "    return round(max(np.random.normal(mean, sd), 0), 2) #avoid savings being negative & change to 2 decimal places\n",
    "\n",
    "sav['Savings'] = sav['Cluster'].apply(sample)\n",
    "\n",
    "# fitting into original\n",
    "original_copy = pd.get_dummies(original_copy, columns=['Income_Category'], drop_first = False)\n",
    "original_copy['Cluster'] = kmeans.predict(original_copy[['Income_Category_40 - 60', 'Income_Category_60 - 80', 'Income_Category_80 - 120', 'Income_Category_120 +']])\n",
    "original_copy['Savings'] = original_copy['Cluster'].apply(sample)\n",
    "original['Savings'] = original_copy['Savings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investment\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "# building fake investment based on their income level and savings\n",
    "def investment(row):\n",
    "    base = faker.random_int(1200, 12000) #invest $100-$1000 per month\n",
    "    if row['Income_Category'] == 'Less than 40':\n",
    "        factor = 1\n",
    "    elif row['Income_Category'] == '40 - 60':\n",
    "        factor = 1.5\n",
    "    elif row['Income_Category'] == '60 - 80':\n",
    "        factor = 2\n",
    "    elif row['Income_Category'] == '80 - 120':\n",
    "        factor = 2\n",
    "    elif row['Income_Category'] == '120 +':\n",
    "        factor = 2.5\n",
    "    # take into account their savings\n",
    "    save = min(row['Savings']/10000, 5)\n",
    "    if row['Savings'] == 0.00:\n",
    "        save = 1 #they can still invest with no savings\n",
    "    # if their savings is small: using a basing investment off a portion of that\n",
    "    investment = base * factor * save\n",
    "    return round(investment, 2)\n",
    "\n",
    "original['Investment'] = original.apply(investment, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model, KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fee Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liviajaison/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/liviajaison/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries([sampled_loan, sampled_balance])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Add fake data to the original dataset\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m original[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampled_outstanding_loan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampled_account_balance\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m original\u001b[38;5;241m.\u001b[39mapply(sample_fake_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(original\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(v)\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m, in \u001b[0;36msample_fake_data\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_fake_data\u001b[39m(row):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Assume a placeholder for fee sensitivity since it's not in original\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     cluster_label \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mpredict([[row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfee_sensitivity\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)]])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m     mean_loan, std_loan \u001b[38;5;241m=\u001b[39m clusters_stats[cluster_label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutstanding_loan\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m     mean_balance, std_balance \u001b[38;5;241m=\u001b[39m clusters_stats[cluster_label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_balance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1097\u001b[0m, in \u001b[0;36m_BaseKMeans.predict\u001b[0;34m(self, X, sample_weight)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the closest cluster each sample in X belongs to.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03mIn the vector quantization literature, `cluster_centers_` is called\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1097\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_test_data(X)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(sample_weight, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m sample_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1099\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1100\u001b[0m         (\n\u001b[1;32m   1101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was deprecated in version 1.3 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1105\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:950\u001b[0m, in \u001b[0;36m_BaseKMeans._check_test_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_test_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 950\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    951\u001b[0m         X,\n\u001b[1;32m    952\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    953\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    954\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[1;32m    955\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    956\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    957\u001b[0m     )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "def generate_fake_data(num_samples):\n",
    "    fake_data = []\n",
    "    for _ in range(num_samples):\n",
    "        loan_amount = np.random.randint(100, 10000)  # Random loan amount between 100 and 10,000\n",
    "        account_balance = np.random.randint(0, 20000)  # Random balance between 0 and 20,000\n",
    "        fee_sensitivity = np.random.uniform(1.0, 5.0)  # Random fee sensitivity between 1 and 5\n",
    "        fake_data.append({\n",
    "            'fee_sensitivity': fee_sensitivity,\n",
    "            'outstanding_loan': loan_amount,\n",
    "            'account_balance': account_balance\n",
    "        })\n",
    "    return pd.DataFrame(fake_data)\n",
    "\n",
    "new_data = generate_fake_data(100)\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=3)  # I chose 3 clusters\n",
    "new_data['cluster'] = kmeans.fit_predict(new_data[['fee_sensitivity']])\n",
    "\n",
    "# Normal distributions for each cluster\n",
    "clusters_stats = {}\n",
    "for cluster in new_data['cluster'].unique():\n",
    "    cluster_data = new_data[new_data['cluster'] == cluster]\n",
    "    mean_loan = cluster_data['outstanding_loan'].mean()\n",
    "    std_loan = cluster_data['outstanding_loan'].std()\n",
    "    mean_balance = cluster_data['account_balance'].mean()\n",
    "    std_balance = cluster_data['account_balance'].std()\n",
    "    clusters_stats[cluster] = {\n",
    "        'outstanding_loan': (mean_loan, std_loan),\n",
    "        'account_balance': (mean_balance, std_balance)\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_fake_data(row):\n",
    "    # Assume a placeholder for fee sensitivity since it's not in original\n",
    "    cluster_label = kmeans.predict([[row.get('fee_sensitivity', np.nan)]])[0]\n",
    "    mean_loan, std_loan = clusters_stats[cluster_label]['outstanding_loan']\n",
    "    mean_balance, std_balance = clusters_stats[cluster_label]['account_balance']\n",
    "    \n",
    "    # Sample from the normal distribution\n",
    "    sampled_loan = np.random.normal(mean_loan, std_loan)\n",
    "    sampled_balance = np.random.normal(mean_balance, std_balance)\n",
    "    \n",
    "    return pd.Series([sampled_loan, sampled_balance])\n",
    "\n",
    "# Add fake data to the original dataset\n",
    "original[['sampled_outstanding_loan', 'sampled_account_balance']] = original.apply(sample_fake_data, axis=1)\n",
    "\n",
    "print(original.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Campaign Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign = pd.read_csv(\"../data/campaign_data.csv\", sep = ';') \n",
    "# https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets?select=test.csv\n",
    "# training data was used since it was much larger than test data (randomly selected rows from training data)\n",
    "\n",
    "# data that will be added\n",
    "# campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "# pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
    "# previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "# poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
    "# y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "\n",
    "# campaign (using normal d/b)\n",
    "np.random.seed(10) # setting seed for reproducibility\n",
    "mean_campaign = campaign['campaign'].mean()\n",
    "sd_campaign = campaign['campaign'].std()\n",
    "n = len(original)\n",
    "original['Num_of_Contacts_Made'] = np.random.normal(mean_campaign, sd_campaign, n)\n",
    "original['Num_of_Contacts_Made'] = np.round(np.maximum(original['Num_of_Contacts_Made'], 0), 0)\n",
    "\n",
    "# pdays (using normal d/b)\n",
    "np.random.seed(10) # setting seed for reproducibility\n",
    "mean_pdays = campaign['pdays'].mean()\n",
    "sd_pdays = campaign['pdays'].std()\n",
    "original['Last_Contacted'] = np.random.normal(mean_pdays, sd_pdays, n)\n",
    "original['Last_Contacted'] = np.round(np.maximum(original['Last_Contacted'], -1), 0)\n",
    "\n",
    "# previous (using normal d/b)\n",
    "np.random.seed(10) # setting seed for reproducibility\n",
    "mean_previous = campaign['previous'].mean()\n",
    "sd_previous = campaign['previous'].std()\n",
    "original['Last_Campaign_Contact'] = np.random.normal(mean_previous, sd_previous, n)\n",
    "original['Last_Campaign_Contact'] = np.round(np.maximum(original['Last_Campaign_Contact'], 0), 0)\n",
    "\n",
    "# poutcomes\n",
    "poutcome_counts = campaign['poutcome'].value_counts()\n",
    "poutcomes = poutcome_counts.index\n",
    "n_campaign = len(campaign)\n",
    "p_poutcome = (poutcome_counts/n_campaign).tolist()\n",
    "original['Last_Campaign_Outcome'] = np.random.choice(poutcomes, size = n, p = p_poutcome)\n",
    "\n",
    "# y \n",
    "y_counts = campaign['y'].value_counts()\n",
    "y_outcomes = y_counts.index\n",
    "p_y = (y_counts/n_campaign).tolist()\n",
    "original['Outcome'] = np.random.choice(y_outcomes, size = n, p = p_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital Banking Behaviour\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/mikhail1681/user-churn\n",
    "\n",
    "relevant features: PhoneService, InternetService, TechSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical = pd.read_csv(\"../data/User churn.csv\")\n",
    "\n",
    "# print(original.head())\n",
    "# original.info()\n",
    "technical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common features\n",
    "common_features = ['gender', 'SeniorCitizen', 'Partner', 'Churn', 'Dependents']\n",
    "\n",
    "# Calculate the distribution of PhoneService\n",
    "phone_service_distribution = (\n",
    "    technical.groupby(common_features)['PhoneService']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Calculate the distribution of InternetService\n",
    "internet_service_distribution = (\n",
    "    technical.groupby(common_features)['InternetService']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Calculate the distribution of TechSupport\n",
    "tech_support_distribution = (\n",
    "    technical.groupby(common_features)['TechSupport']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "print(phone_service_distribution)\n",
    "print(internet_service_distribution)\n",
    "print(tech_support_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original['gender'] = original['Gender'].apply(lambda x: 'Male' if x == 'M' else 'Female')\n",
    "original['SeniorCitizen'] = (original['Customer_Age'] > 60).astype(int)\n",
    "original['Partner'] = original['Marital_Status'].apply(lambda x: 'Yes' if x == 'Married' else 'No')\n",
    "original['Dependents'] = original['Dependent_count'].apply(lambda x: 'Yes' if x > 0 else 'No')\n",
    "original['Churn'] = original['Attrition_Flag'].apply(lambda x: 'Yes' if x == 'Existing Customer' else 'No')\n",
    "\n",
    "original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_features(df, distribution, feature):\n",
    "    synthetic_feature = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        features = tuple(row[common_features])\n",
    "        \n",
    "            # Use the distribution to sample\n",
    "        sample = np.random.choice(\n",
    "            distribution.columns,\n",
    "            p=distribution.loc[features].values\n",
    "        )\n",
    "        synthetic_feature.append(sample)\n",
    "            \n",
    "    return synthetic_feature\n",
    "\n",
    "# Generate synthetic features\n",
    "original['PhoneService'] = generate_synthetic_features(original, phone_service_distribution, 'PhoneService')\n",
    "original['InternetService'] = generate_synthetic_features(original, internet_service_distribution, 'InternetService')\n",
    "original['TechSupport'] = generate_synthetic_features(original, tech_support_distribution, 'TechSupport')\n",
    "\n",
    "original.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
