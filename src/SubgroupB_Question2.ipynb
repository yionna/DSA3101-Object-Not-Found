{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What strategies can we implement to optimize our marketing campaigns in real-time?\n",
    "- Create an algorithm for dynamic campaign adjustment based on real-time performance\n",
    "metrics.\n",
    "- Simulate the impact of proposed adjustments on campaign effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "CODE WAIT TO BE ADJUSTED BASED ON THE RECOMMENDATION SYSTEM OUTCOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoadMap\n",
    "1. Data Cleaning\n",
    "2. Data Merging\n",
    "3. Online Learning with SGD\n",
    "4. Simulation with A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data = pd.read_csv(\"../data/bank_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         TransactionID CustomerID CustomerDOB CustGender   CustLocation  \\\n",
       "0                  T1   C5841053     10/1/94          F     JAMSHEDPUR   \n",
       "1                  T2   C2142763      4/4/57          M        JHAJJAR   \n",
       "2                  T3   C4417068    26/11/96          F         MUMBAI   \n",
       "3                  T4   C5342380     14/9/73          F         MUMBAI   \n",
       "4                  T5   C9031234     24/3/88          F    NAVI MUMBAI   \n",
       "...               ...        ...         ...        ...            ...   \n",
       "1048562      T1048563   C8020229      8/4/90          M      NEW DELHI   \n",
       "1048563      T1048564   C6459278     20/2/92          M         NASHIK   \n",
       "1048564      T1048565   C6412354     18/5/89          M      HYDERABAD   \n",
       "1048565      T1048566   C6420483     30/8/78          M  VISAKHAPATNAM   \n",
       "1048566      T1048567   C8337524      5/3/84          M           PUNE   \n",
       "\n",
       "         CustAccountBalance TransactionDate  TransactionTime  \\\n",
       "0                  17819.05          2/8/16           143207   \n",
       "1                   2270.69          2/8/16           141858   \n",
       "2                  17874.44          2/8/16           142712   \n",
       "3                 866503.21          2/8/16           142714   \n",
       "4                   6714.43          2/8/16           181156   \n",
       "...                     ...             ...              ...   \n",
       "1048562             7635.19         18/9/16           184824   \n",
       "1048563            27311.42         18/9/16           183734   \n",
       "1048564           221757.06         18/9/16           183313   \n",
       "1048565            10117.87         18/9/16           184706   \n",
       "1048566            75734.42         18/9/16           181222   \n",
       "\n",
       "         TransactionAmount (INR)  \n",
       "0                           25.0  \n",
       "1                        27999.0  \n",
       "2                          459.0  \n",
       "3                         2060.0  \n",
       "4                         1762.5  \n",
       "...                          ...  \n",
       "1048562                    799.0  \n",
       "1048563                    460.0  \n",
       "1048564                    770.0  \n",
       "1048565                   1000.0  \n",
       "1048566                   1166.0  \n",
       "\n",
       "[1048567 rows x 9 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicate\n",
    "transaction_data = transaction_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID                 0\n",
       "CustomerID                    0\n",
       "CustomerDOB                3397\n",
       "CustGender                 1100\n",
       "CustLocation                151\n",
       "CustAccountBalance         2369\n",
       "TransactionDate               0\n",
       "TransactionTime               0\n",
       "TransactionAmount (INR)       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are dealing with banking campaign instead of fraud detection, so we consider that CustLocation is not a useful feature\n",
    "transaction_data = transaction_data.drop('CustLocation', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## given that this data is of customer transaction, we can see if the missing value is for just some particular customer\n",
    "columns_to_check = ['CustomerDOB', 'CustGender', 'CustAccountBalance']\n",
    "customers_with_missing_data = transaction_data[transaction_data[columns_to_check].isna().any(axis=1)]['CustomerID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique CustomerIDs: 884265\n"
     ]
    }
   ],
   "source": [
    "unique_customer_ids = transaction_data['CustomerID'].unique()\n",
    "print(f\"Number of unique CustomerIDs: {len(unique_customer_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_missing_data = transaction_data[transaction_data[columns_to_check].isna().any(axis=1)]\n",
    "all_transactions_for_customers = transaction_data[transaction_data['CustomerID'].isin(customers_with_missing_data)]\n",
    "other_transactions = all_transactions_for_customers[~all_transactions_for_customers['TransactionID'].isin(transactions_with_missing_data['TransactionID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique CustomerIDs: 1990\n"
     ]
    }
   ],
   "source": [
    "## we can remove those customers that only have transaction with missing data consider their size is more relative to the whole dataset\n",
    "customers_with_other_transactions = other_transactions['CustomerID'].unique()\n",
    "print(f\"Number of unique CustomerIDs: {len(customers_with_other_transactions)}\")\n",
    "customers_with_only_missing_transactions = transactions_with_missing_data[\n",
    "    ~transactions_with_missing_data['CustomerID'].isin(customers_with_other_transactions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_with_only_missing_transactions = customers_with_only_missing_transactions['CustomerID'].unique()\n",
    "transaction_data = transaction_data[~transaction_data['CustomerID'].isin(customers_with_only_missing_transactions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we try to impute missing data based on the other transaction of the same customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique CustomerIDs: 879472\n"
     ]
    }
   ],
   "source": [
    "unique_customer_ids = transaction_data['CustomerID'].unique()\n",
    "print(f\"Number of unique CustomerIDs: {len(unique_customer_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13081\\AppData\\Local\\Temp\\ipykernel_23188\\452474458.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  transaction_data_with = grouped.apply(lambda x: x.sort_values('TransactionID_numeric'))\n"
     ]
    }
   ],
   "source": [
    "## we generally assume that transaction later would have a transaction ID with larger number\n",
    "\n",
    "transaction_data_without = transaction_data[~transaction_data['CustomerID'].isin(customers_with_other_transactions)].copy()\n",
    "transaction_data_with = transaction_data[transaction_data['CustomerID'].isin(customers_with_other_transactions)].copy()\n",
    "\n",
    "transaction_data_with['TransactionID_numeric'] = transaction_data_with['TransactionID'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "grouped = transaction_data_with.groupby('CustomerID', group_keys=False)\n",
    "transaction_data_with = grouped.apply(lambda x: x.sort_values('TransactionID_numeric'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_with['CustomerDOB'] = grouped['CustomerDOB'].apply(lambda x: x.ffill().bfill())\n",
    "transaction_data_with['CustGender'] = grouped['CustGender'].apply(lambda x: x.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID                0\n",
       "CustomerID                   0\n",
       "CustomerDOB                  0\n",
       "CustGender                   0\n",
       "CustAccountBalance         702\n",
       "TransactionDate              0\n",
       "TransactionTime              0\n",
       "TransactionAmount (INR)      0\n",
       "TransactionID_numeric        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data_with = transaction_data_with.reset_index(drop=True)\n",
    "transaction_data_with.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13081\\AppData\\Local\\Temp\\ipykernel_23188\\1759441042.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  transaction_data_with = grouped.apply(impute_balance)\n"
     ]
    }
   ],
   "source": [
    "def impute_balance(group):\n",
    "    # Ensure transactions are sorted by TransactionID or TransactionID_numeric\n",
    "    group = group.sort_values('TransactionID_numeric')\n",
    "    \n",
    "    # Forward fill and backward fill for known balance values (handles missing values at the start and end)\n",
    "    group['CustAccountBalance'] = group['CustAccountBalance'].ffill().bfill()\n",
    "    \n",
    "    # Loop through transactions and impute missing balances based on previous balance and TransactionAmount\n",
    "    for i in range(1, len(group)):\n",
    "        if pd.isna(group.iloc[i]['CustAccountBalance']):\n",
    "            group.iloc[i, group.columns.get_loc('CustAccountBalance')] = (\n",
    "                group.iloc[i-1]['CustAccountBalance'] + group.iloc[i]['TransactionAmount (INR)']\n",
    "            )\n",
    "    \n",
    "    return group\n",
    "\n",
    "grouped = transaction_data_with.groupby('CustomerID', group_keys=False)\n",
    "transaction_data_with = grouped.apply(impute_balance)\n",
    "\n",
    "missing_balances_after_imputation = transaction_data_with[transaction_data_with['CustAccountBalance'].isna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_with = transaction_data_with.drop(\"TransactionID_numeric\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_cleaned = pd.concat([transaction_data_without,transaction_data_with], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_cleaned.loc[:,'CustGender'] = transaction_data_cleaned.loc[:,'CustGender'].replace({\"F\":1, \"M\":0})\n",
    "transaction_data_cleaned['TransactionDate'] = pd.to_datetime(transaction_data_cleaned['TransactionDate'], errors='coerce', format='%d/%m/%y') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_cleaned['CustomerDOB'] = transaction_data_cleaned['CustomerDOB'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = transaction_data_cleaned.copy()\n",
    "temp['CustomerDOB'] = pd.to_datetime(temp['CustomerDOB'], format='%d/%m/%y', errors='coerce')\n",
    "nullindex = temp[temp['CustomerDOB'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16         1/1/1800\n",
       "22         1/1/1800\n",
       "28         1/1/1800\n",
       "34         1/1/1800\n",
       "150        1/1/1800\n",
       "             ...   \n",
       "1043696    1/1/1800\n",
       "1043704    1/1/1800\n",
       "1043705    1/1/1800\n",
       "1043725    1/1/1800\n",
       "1043739    1/1/1800\n",
       "Name: CustomerDOB, Length: 56633, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data_cleaned['CustomerDOB'][nullindex] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### it's easy to find that they dont have DOB record for those row, we can only remove them\n",
    "transaction_data_cleaned['CustomerDOB'] = pd.to_datetime(transaction_data_cleaned['CustomerDOB'], format='%d/%m/%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_cleaned['CustomerDOB'] = transaction_data_cleaned['CustomerDOB'].apply(lambda x: x if x.year <= 2024 else x.replace(year=x.year - 100))\n",
    "transaction_data_cleaned = transaction_data_cleaned.dropna(subset=['CustomerDOB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_cleaned['TransactionTime'] = transaction_data_cleaned['TransactionTime'].astype(str).str.zfill(6)\n",
    "transaction_data_cleaned['TransactionTime'] = transaction_data_cleaned['TransactionTime'].apply(lambda x: datetime.strptime(x, '%H%M%S').time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 987133 entries, 0 to 1043765\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count   Dtype         \n",
      "---  ------                   --------------   -----         \n",
      " 0   TransactionID            987133 non-null  object        \n",
      " 1   CustomerID               987133 non-null  object        \n",
      " 2   CustomerDOB              987133 non-null  datetime64[ns]\n",
      " 3   CustGender               987133 non-null  object        \n",
      " 4   CustAccountBalance       987133 non-null  float64       \n",
      " 5   TransactionDate          987133 non-null  datetime64[ns]\n",
      " 6   TransactionTime          987133 non-null  object        \n",
      " 7   TransactionAmount (INR)  987133 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(2), object(4)\n",
      "memory usage: 67.8+ MB\n"
     ]
    }
   ],
   "source": [
    "transaction_data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(transaction_date, dob):\n",
    "    if pd.isna(transaction_date) or pd.isna(dob):\n",
    "        return None  # Return None if either date is missing\n",
    "    age = transaction_date.year - dob.year - ((transaction_date.month, transaction_date.day) < (dob.month, dob.day))\n",
    "    return age\n",
    "\n",
    "# Step 3: Apply the function row-wise to calculate 'CustAge'\n",
    "transaction_data_cleaned['CustAge'] = transaction_data_cleaned.apply(lambda row: calculate_age(row['TransactionDate'], row['CustomerDOB']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data = transaction_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign = pd.read_csv(\"../data/campaign_data.csv\", delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          0\n",
       "job          0\n",
       "marital      0\n",
       "education    0\n",
       "default      0\n",
       "balance      0\n",
       "housing      0\n",
       "loan         0\n",
       "contact      0\n",
       "day          0\n",
       "month        0\n",
       "duration     0\n",
       "campaign     0\n",
       "pdays        0\n",
       "previous     0\n",
       "poutcome     0\n",
       "y            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign  = campaign.drop_duplicates()\n",
    "campaign.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign = campaign.rename({\"y\": \"outcome\"},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column age:\n",
      "[58 44 33 47 35 28 42 43 41 29 53 57 51 45 60 56 32 25 40 39 52 46 36 49\n",
      " 59 37 50 54 55 48 24 38 31 30 27 34 23 26 61 22 21 20 66 62 83 75 67 70\n",
      " 65 68 64 69 72 71 19 76 85 63 90 82 73 74 78 80 94 79 77 86 95 81 18 89\n",
      " 84 87 92 93 88]\n",
      "\n",
      "\n",
      "Unique values in column job:\n",
      "['management' 'technician' 'entrepreneur' 'blue-collar' 'unknown'\n",
      " 'retired' 'admin.' 'services' 'self-employed' 'unemployed' 'housemaid'\n",
      " 'student']\n",
      "\n",
      "\n",
      "Unique values in column marital:\n",
      "['married' 'single' 'divorced']\n",
      "\n",
      "\n",
      "Unique values in column education:\n",
      "['tertiary' 'secondary' 'unknown' 'primary']\n",
      "\n",
      "\n",
      "Unique values in column default:\n",
      "['no' 'yes']\n",
      "\n",
      "\n",
      "Unique values in column balance:\n",
      "[ 2143    29     2 ...  8205 14204 16353]\n",
      "\n",
      "\n",
      "Unique values in column housing:\n",
      "['yes' 'no']\n",
      "\n",
      "\n",
      "Unique values in column loan:\n",
      "['no' 'yes']\n",
      "\n",
      "\n",
      "Unique values in column contact:\n",
      "['unknown' 'cellular' 'telephone']\n",
      "\n",
      "\n",
      "Unique values in column day:\n",
      "[ 5  6  7  8  9 12 13 14 15 16 19 20 21 23 26 27 28 29 30  2  3  4 11 17\n",
      " 18 24 25  1 10 22 31]\n",
      "\n",
      "\n",
      "Unique values in column month:\n",
      "['may' 'jun' 'jul' 'aug' 'oct' 'nov' 'dec' 'jan' 'feb' 'mar' 'apr' 'sep']\n",
      "\n",
      "\n",
      "Unique values in column duration:\n",
      "[ 261  151   76 ... 1298 1246 1556]\n",
      "\n",
      "\n",
      "Unique values in column campaign:\n",
      "[ 1  2  3  5  4  6  7  8  9 10 11 12 13 19 14 24 16 32 18 22 15 17 25 21\n",
      " 43 51 63 41 26 28 55 50 38 23 20 29 31 37 30 46 27 58 33 35 34 36 39 44]\n",
      "\n",
      "\n",
      "Unique values in column pdays:\n",
      "[ -1 151 166  91  86 143 147  89 140 176 101 174 170 167 195 165 129 188\n",
      " 196 172 118 119 104 171 117 164 132 131 123 159 186 111 115 116 173 178\n",
      " 110 152  96 103 150 175 193 181 185 154 145 138 126 180 109 158 168  97\n",
      " 182 127 130 194 125 105 102  26 179  28 183 155 112 120 137 124 187 190\n",
      " 113 162 134 169 189   8 144 191 184 177   5  99 133  93  92  10 100 156\n",
      " 198 106 153 146 128   7 121 160 107  90  27 197 136 139 122 157 149 135\n",
      "  30 114  98 192 163  34  95 141  31 199  94 108  29 268 247 253 226 244\n",
      " 239 245 204 231 238 258 230 254 265  71 223 246 250 266 240 205 261 259\n",
      " 241 260 234 251 225 161 237 262 248 255 220 227 206 224 249 235 228 263\n",
      "   2 270 232 252 207 200 269 233 256 273 272 242 264 208 214 222 271 203\n",
      " 221 202 216 201 257 229 210 217  75 213  73  76 267 211 215  77 236  82\n",
      "   6 209 274   1 243 212 275  80 276   9 279  12 280  88 277  85  84 219\n",
      "  24  21 282  41 294  49 329 307 303 331 308 300  64 314 287 330 332 302\n",
      " 323 318 333  60 326 335 313 312 305 325 327 336 309 328 322  39 316 292\n",
      " 295 310 306 320 317 289  57 321 142 339 301 315 337 334 340 319  17  74\n",
      " 148 341 299 344 342 324 345 346 304 281 343 338  14 347  15 291 348 349\n",
      " 285 350 284  25 283 278  81   4  87  83  79  70  13 293  37  78  63  22\n",
      " 296 355  66  19  35 360 357 354 351 362 358 365 298 286 364 363  47 361\n",
      " 288 366 356 352 359 297 367 353 368  42 290  67 371 370 369  50  36 373\n",
      " 374 372 311 375 378  59 379  40  18  43  20  69  38 385  56  55  44 391\n",
      "  72 390  32  62 399 393  65 377 395 388 389 386  61 412 405 434 394 382\n",
      " 459 440 397 383  68 461 462 463 422  51 457 430 442 403 454 428 392 410\n",
      " 401 474 475 477 478  54 476 380 479  45  46 495  58  48 518  52 515 520\n",
      " 511 536 387 218  33 544 435 436 555 433 446 558 469 616 561 553 384 592\n",
      " 467 585 480 421 667 626 426 595 381 376 648 521 452 449 633 398  53 460\n",
      " 670 551 414 557 687 404 651 686 425 504 578 674 416 586 411 756 450 745\n",
      " 514 417 424 776 396 683 529 439 415 456 407 458 532 481 791 701 531 792\n",
      " 413 445 535 784 419 455 491 431 542 470 472 717 437   3 782 728 828 524\n",
      " 562 761 492 775 579 493 464 760 466 465 656 831 490 432 655 427 749 838\n",
      " 769 587 778 854 779 850 771 594 842 589 603 484 489 486 409 444 680 808\n",
      " 485 503 690 772 774 526 420 528 500 826 804 508 547 805 541 543 871 550\n",
      " 530]\n",
      "\n",
      "\n",
      "Unique values in column previous:\n",
      "[  0   3   1   4   2  11  16   6   5  10  12   7  18   9  21   8  14  15\n",
      "  26  37  13  25  20  27  17  23  38  29  24  51 275  22  19  30  58  28\n",
      "  32  40  55  35  41]\n",
      "\n",
      "\n",
      "Unique values in column poutcome:\n",
      "['unknown' 'failure' 'other' 'success']\n",
      "\n",
      "\n",
      "Unique values in column outcome:\n",
      "['no' 'yes']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_values = {}\n",
    "for col in campaign.columns:\n",
    "    unique_values[col] = campaign[col].unique()\n",
    "for col, values in unique_values.items():\n",
    "    print(f\"Unique values in column {col}:\")\n",
    "    print(values)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## null values are in the form of \"unknown\"\n",
    "## according to the feature description, \"other\" is considered \"nonexistent\" - that is: they dont have last campaign\n",
    "campaign['poutcome'] = campaign['poutcome'].replace({\"other\":\"unexistent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign['month'] = pd.to_datetime(campaign['month'], format='%b').dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we convert unknown to null values\n",
    "campaign['contact'] = campaign['contact'].replace({\"unknown\":np.nan})\n",
    "campaign['education'] = campaign['education'].replace({\"unknown\":np.nan})\n",
    "campaign['job'] = campaign['job'].replace({\"unknown\":np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age              0\n",
       "job            288\n",
       "marital          0\n",
       "education     1857\n",
       "default          0\n",
       "balance          0\n",
       "housing          0\n",
       "loan             0\n",
       "contact      13020\n",
       "day              0\n",
       "month            0\n",
       "duration         0\n",
       "campaign         0\n",
       "pdays            0\n",
       "previous         0\n",
       "poutcome         0\n",
       "outcome          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## since the missing rows of job and education are of small size compared to the whole dataset, we can simply drop it\n",
    "## since campaign can only be conducted based on having contact, so we will impute the mode value(given that they all have last contact)\n",
    "campaign['contact'] = campaign['contact'].fillna(campaign['contact'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign = campaign.dropna(subset=['job'])\n",
    "campaign = campaign.dropna(subset=['education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert job to income group based on assumption on jobs\n",
    "job_to_income_group = {\n",
    "    'management': '$120K +',\n",
    "    'entrepreneur': '$120K +',\n",
    "    'self-employed': '$120K +',\n",
    "    'technician': '$80K - $120K',\n",
    "    'admin.': '$80K - $120K',\n",
    "    'services': '$60K - $80K',\n",
    "    'blue-collar': '$40K - $60K',\n",
    "    'housemaid': '$40K - $60K',\n",
    "    'unemployed': 'Less than $40K',\n",
    "    'student': 'Less than $40K',\n",
    "    'retired': 'Less than $40K'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign.loc[:,\"income\"] =campaign['job'].map(job_to_income_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_data = campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerDOB</th>\n",
       "      <th>CustGender</th>\n",
       "      <th>CustAccountBalance</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>TransactionTime</th>\n",
       "      <th>TransactionAmount (INR)</th>\n",
       "      <th>CustAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>C5841053</td>\n",
       "      <td>1994-01-10</td>\n",
       "      <td>1</td>\n",
       "      <td>17819.05</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>14:32:07</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>C2142763</td>\n",
       "      <td>1957-04-04</td>\n",
       "      <td>0</td>\n",
       "      <td>2270.69</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>14:18:58</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>C4417068</td>\n",
       "      <td>1996-11-26</td>\n",
       "      <td>1</td>\n",
       "      <td>17874.44</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>14:27:12</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>C5342380</td>\n",
       "      <td>1973-09-14</td>\n",
       "      <td>1</td>\n",
       "      <td>866503.21</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>14:27:14</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>C9031234</td>\n",
       "      <td>1988-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>6714.43</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>18:11:56</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043761</th>\n",
       "      <td>T1047249</td>\n",
       "      <td>C8510525</td>\n",
       "      <td>1992-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>98896.96</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>00:58:52</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043762</th>\n",
       "      <td>T1047276</td>\n",
       "      <td>C1640779</td>\n",
       "      <td>1991-11-02</td>\n",
       "      <td>1</td>\n",
       "      <td>4410.16</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>07:25:27</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043763</th>\n",
       "      <td>T1047576</td>\n",
       "      <td>C6119081</td>\n",
       "      <td>1984-02-23</td>\n",
       "      <td>0</td>\n",
       "      <td>75947.08</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>17:48:38</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043764</th>\n",
       "      <td>T1047763</td>\n",
       "      <td>C3827041</td>\n",
       "      <td>1995-05-18</td>\n",
       "      <td>0</td>\n",
       "      <td>91.36</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>19:31:22</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043765</th>\n",
       "      <td>T1048050</td>\n",
       "      <td>C8933061</td>\n",
       "      <td>1981-04-19</td>\n",
       "      <td>0</td>\n",
       "      <td>327054.31</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>08:24:49</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987133 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID CustomerID CustomerDOB CustGender  CustAccountBalance  \\\n",
       "0                  T1   C5841053  1994-01-10          1            17819.05   \n",
       "1                  T2   C2142763  1957-04-04          0             2270.69   \n",
       "2                  T3   C4417068  1996-11-26          1            17874.44   \n",
       "3                  T4   C5342380  1973-09-14          1           866503.21   \n",
       "4                  T5   C9031234  1988-03-24          1             6714.43   \n",
       "...               ...        ...         ...        ...                 ...   \n",
       "1043761      T1047249   C8510525  1992-04-10          0            98896.96   \n",
       "1043762      T1047276   C1640779  1991-11-02          1             4410.16   \n",
       "1043763      T1047576   C6119081  1984-02-23          0            75947.08   \n",
       "1043764      T1047763   C3827041  1995-05-18          0               91.36   \n",
       "1043765      T1048050   C8933061  1981-04-19          0           327054.31   \n",
       "\n",
       "        TransactionDate TransactionTime  TransactionAmount (INR)  CustAge  \n",
       "0            2016-08-02        14:32:07                 0.000016       22  \n",
       "1            2016-08-02        14:18:58                 0.017948       59  \n",
       "2            2016-08-02        14:27:12                 0.000294       19  \n",
       "3            2016-08-02        14:27:14                 0.001320       42  \n",
       "4            2016-08-02        18:11:56                 0.001130       28  \n",
       "...                 ...             ...                      ...      ...  \n",
       "1043761      2016-09-18        00:58:52                 0.001923       24  \n",
       "1043762      2016-09-18        07:25:27                 0.000044       24  \n",
       "1043763      2016-09-18        17:48:38                 0.000160       32  \n",
       "1043764      2016-09-18        19:31:22                 0.000182       21  \n",
       "1043765      2016-09-18        08:24:49                 0.001923       35  \n",
       "\n",
       "[987133 rows x 9 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building planning \n",
    "1. what kind of data we need:\n",
    "    - real-time performance metrics\n",
    "    - demographic features\n",
    "    - financial bahaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real time performance metrics:\n",
    "- `click-through rate` number of successful outcomes per campaign\n",
    "- `conversion rate` number of desired action (making transaction) per campaign\n",
    "- `transaction frequency` number of transaction from past month\n",
    "\n",
    "2. What we need to do:\n",
    "    We now have two separate data - campaign and transaction data\n",
    "    We need time-series data that consistently updated the outcome and the number of transaction\n",
    "    For simplicity, we decide to randomly assign the possible outcome and number of campaign based on the distribution\n",
    "    - distribution of campaign number\n",
    "    - for each campaign number, there is corresponding percentage of outcome (failure vs success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID              0\n",
       "CustomerID                 0\n",
       "CustomerDOB                0\n",
       "CustGender                 0\n",
       "CustAccountBalance         0\n",
       "TransactionDate            0\n",
       "TransactionTime            0\n",
       "TransactionAmount (INR)    0\n",
       "CustAge                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data['CustAge'] = transaction_data['CustAge'].astype(int)\n",
    "transaction_data['TransactionDate'] = pd.to_datetime(transaction_data['TransactionDate'])\n",
    "transaction_data['TransactionAmount (INR)'] = MinMaxScaler().fit_transform(transaction_data[['TransactionAmount (INR)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_data['day'] = campaign_data['day'].astype(int)\n",
    "campaign_data['duration'] = campaign_data['duration'].astype(int)\n",
    "le_contact = LabelEncoder()\n",
    "le_outcome = LabelEncoder()\n",
    "campaign_data['contact_encoded'] = le_contact.fit_transform(campaign_data['contact'])\n",
    "campaign_data['outcome_encoded'] = le_outcome.fit_transform(campaign_data['outcome'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_distribution = campaign_data['campaign'].value_counts(normalize=True)  \n",
    "outcome_distribution = campaign_data.groupby('campaign')['outcome'].value_counts(normalize=True).unstack(fill_value=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def generate_context_outcome(n_samples, context_dist, outcome_dist):\n",
    "    number_of_campaigns = np.random.choice(context_dist.index, size=n_samples, p=context_dist.values)\n",
    "    \n",
    "    outcomes = []\n",
    "    for context in number_of_campaigns:\n",
    "        outcome_probs = outcome_dist.loc[context]\n",
    "        outcome = np.random.choice(outcome_probs.index, p=outcome_probs.values)\n",
    "        outcomes.append(outcome)\n",
    "        \n",
    "    return number_of_campaigns, outcomes\n",
    "\n",
    "n_samples = len(transaction_data)\n",
    "batch_size = 10000 \n",
    "n_batches = n_samples // batch_size + 1\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(generate_context_outcome)(batch_size, context_distribution, outcome_distribution) for _ in range(n_batches))\n",
    "\n",
    "number_of_campaigns = np.concatenate([result[0] for result in results])\n",
    "outcomes = np.concatenate([result[1] for result in results])\n",
    "\n",
    "transaction_data['number_of_campaigns'] = number_of_campaigns[:n_samples]\n",
    "transaction_data['outcome'] = outcomes[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using multi-arm bandit as the algorithm for reinforcement learning\n",
    "- actions = [change offers, change campaign timing]\n",
    "- implemented recommendation system for changing offers\n",
    "- (possible) implemented segmentation update for recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff_date = transaction_data['TransactionDate'].quantile(0.7)\n",
    "train_data = transaction_data[transaction_data['TransactionDate'] <= train_cutoff_date]\n",
    "test_data = transaction_data[transaction_data['TransactionDate'] > train_cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CustomerID  transaction_frequency  click_through_rate  conversion_rate  \\\n",
      "171876   C1010011                    1.0                 0.0         1.000000   \n",
      "359631   C1010012                    2.0                 0.0         0.333333   \n",
      "88709    C1010014                    3.0                 0.0         1.000000   \n",
      "249304   C1010014                    4.0                 0.0         2.000000   \n",
      "397693   C1010024                    5.0                 0.0         1.000000   \n",
      "\n",
      "        engagement_score  \n",
      "171876               0.5  \n",
      "359631               0.5  \n",
      "88709                0.9  \n",
      "249304               1.4  \n",
      "397693               1.3  \n",
      "       CustomerID  transaction_frequency  click_through_rate  conversion_rate  \\\n",
      "33406    C1010011                    1.0                 0.0              0.5   \n",
      "963368   C1010018                    2.0                 0.0              1.0   \n",
      "889959   C1010038                    3.0                 0.0              0.5   \n",
      "880234   C1010041                    4.0                 0.0              2.0   \n",
      "949139   C1010041                    5.0                 0.0              2.0   \n",
      "\n",
      "        engagement_score  \n",
      "33406               0.35  \n",
      "963368              0.70  \n",
      "889959              0.75  \n",
      "880234              1.40  \n",
      "949139              1.60  \n"
     ]
    }
   ],
   "source": [
    "def feature_engineering_optimized(data):\n",
    "    data.loc[:,'TransactionDate'] = pd.to_datetime(data['TransactionDate'])\n",
    "    data = data.sort_values(['CustomerID', 'TransactionDate'])\n",
    "    \n",
    "    data['transaction_diff'] = data.groupby('CustomerID')['TransactionDate'].diff().dt.days.fillna(0)\n",
    "    data['transaction_frequency'] = data['transaction_diff'].rolling(window=30, min_periods=1).apply(lambda x: (x <= 30).sum())\n",
    "\n",
    "    successful_outcomes = data[data['outcome'] == 1].groupby('CustomerID')['outcome'].transform('count')\n",
    "    data['successful_outcomes'] = data['CustomerID'].map(successful_outcomes).fillna(0)\n",
    "    data['click_through_rate'] = data['successful_outcomes'] / data['number_of_campaigns']\n",
    "\n",
    "    transaction_counts = data.groupby('CustomerID')['TransactionID'].transform('count')\n",
    "    data['conversion_rate'] = transaction_counts / data['number_of_campaigns']\n",
    "\n",
    "    w1, w2, w3 = 0.3, 0.5, 0.2 ## wait to be adjusted\n",
    "    data['engagement_score'] = (w1 * data['conversion_rate'] +\n",
    "                                w2 * data['click_through_rate'] +\n",
    "                                w3 * data['transaction_frequency'])\n",
    "\n",
    "    data = data.drop(columns=['transaction_diff'])\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = feature_engineering_optimized(train_data)\n",
    "test_data = feature_engineering_optimized(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = ['conversion_rate', 'click_through_rate', 'transaction_frequency', 'engagement_score']\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_data[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "test_data[features_to_scale] = scaler.transform(test_data[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ['no_offer', 'standard_offer', 'premium_offer']\n",
    "num_actions = len(action_space)\n",
    "preferences = np.zeros(num_actions)  # initialize all actions with equal perference zero\n",
    "probabilities = np.exp(preferences) / np.sum(np.exp(preferences))  # use gradient bandit\n",
    "alpha = 0.1  # learning rate(step size)\n",
    "baseline = 0  \n",
    "\n",
    "def compute_reward(engagement_score):\n",
    "    return engagement_score\n",
    "\n",
    "# train the model\n",
    "for index, row in train_data.iterrows():\n",
    "    action = np.random.choice(action_space, p=probabilities) # choose an action\n",
    "    action_index = action_space.index(action)\n",
    "    \n",
    "    # find reward and update baseline\n",
    "    reward = compute_reward(row['engagement_score'])\n",
    "    baseline = baseline + 0.1 * (reward - baseline) \n",
    "\n",
    "    # update preference\n",
    "    for i in range(num_actions):\n",
    "        if i == action_index:\n",
    "            preferences[i] += alpha * (reward - baseline) * (1 - probabilities[i])\n",
    "        else:\n",
    "            preferences[i] -= alpha * (reward - baseline) * probabilities[i]\n",
    "    \n",
    "    # update action probabilities\n",
    "    probabilities = np.exp(preferences) / np.sum(np.exp(preferences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider that higher engagement score result in higher probability of accepting offer, so the system is built to maximize engagement score. Yet, here we name it as click_through rate to avoid confusion with segmentation rule as in subgroupA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: no_offer, Average Click-Through Rate on Test Data: 0.7798\n",
      "Action: standard_offer, Average Click-Through Rate on Test Data: 0.7798\n",
      "Action: premium_offer, Average Click-Through Rate on Test Data: 0.7797\n"
     ]
    }
   ],
   "source": [
    "# train on test set\n",
    "def evaluate_click_through_rate(test_data, action_space, probabilities):\n",
    "    click_through_results = {action: [] for action in action_space}\n",
    "    \n",
    "    for index, row in test_data.iterrows():\n",
    "        action = np.random.choice(action_space, p=probabilities)\n",
    "        reward = compute_reward(row['engagement_score'])\n",
    "        \n",
    "        if action == 'no_offer':\n",
    "            click_through_results['no_offer'].append(reward)\n",
    "        elif action == 'standard_offer':\n",
    "            click_through_results['standard_offer'].append(reward)\n",
    "        elif action == 'premium_offer':\n",
    "            click_through_results['premium_offer'].append(reward)\n",
    "    \n",
    "    for action, rewards in click_through_results.items():\n",
    "        avg_click_through_rate = np.mean(rewards) if rewards else 0\n",
    "        print(f\"Action: {action}, Average Click-Through Rate on Test Data: {avg_click_through_rate:.4f}\")\n",
    "\n",
    "evaluate_click_through_rate(test_data, action_space, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To build an optimizing algorithm\n",
    "\n",
    "1. Objective and Metrics:\n",
    "The primary objective was to dynamically optimize campaign actions to maximize customer engagement. Although the metric displayed is labeled as `click-through rate` (CTR) for clarity and consistency across segments, it actually represents the average `engagement score`—a composite score that includes `click-through rate`, `conversion rate`, and `transaction frequency`. This allows for a holistic view of campaign effectiveness.\n",
    "\n",
    "2. Adjustment Process:\n",
    "The Gradient Bandit model was applied to adaptively select from three actions: `no_offer`, `standard_offer`, and `premium_offer`. Actions were chosen based on the model’s ongoing evaluation of engagement scores, which provide an aggregated measure of customer responsiveness to different campaign actions.\n",
    "\n",
    "3. Test Data Results and Interpretation:\n",
    "\n",
    "The average engagement score for each action (labeled as \"CTR\" for ease of comparison) was nearly identical, with `no_offer` and `standard_offer` achieving an average score of 0.7798, and `premium_offer` slightly lower at 0.7797.\n",
    "\n",
    "Interpretation: This uniformity across actions suggests that, for the given test data, all actions performed similarly in terms of driving engagement. The small differences imply that no single action was significantly more effective than the others in enhancing customer engagement.\n",
    "\n",
    "4. Implications for Campaign Adjustment:\n",
    "\n",
    "These results suggest that further customization of actions, potentially based on refined customer segmentation, could reveal more meaningful differences in engagement.\n",
    "While each action's impact on engagement was similar in this test, the model's dynamic adjustment capabilities are adaptable and could be highly effective in other scenarios where the variation in engagement across actions is more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given that I dont have exact custome recommendation system, below is a newly built model that wait for the adjustment based on recomendation system, besides, possible segmentation system probably required for adding segmentation system, need someone to deal with the generation. I believe this model would be better compared to the above one that is simplified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_function = False # you will not be able to run it until I fully adjusted it\n",
    "historical_data = pd.DataFrame()  # suppose it is updated by the following transaction\n",
    "\n",
    "# simulate real-time data entries and feature engineering for it\n",
    "def process_transaction_data(new_transaction):\n",
    "    global run_function\n",
    "    \n",
    "    if not run_function:\n",
    "        print(\"The function is currently restrained and will not run.\")\n",
    "        return \n",
    "    global historical_data\n",
    "\n",
    "    new_transaction['TransactionDate'] = pd.to_datetime(new_transaction['TransactionDate'])\n",
    "    # add to the history\n",
    "    historical_data = pd.concat([historical_data, new_transaction], ignore_index=True)\n",
    "    \n",
    "    # sorted by date\n",
    "    historical_data = historical_data.sort_values(['CustomerID', 'TransactionDate']).reset_index(drop=True)\n",
    "    \n",
    "    historical_data['transaction_diff'] = historical_data.groupby('CustomerID')['TransactionDate'].diff().dt.days.fillna(0)\n",
    "    historical_data['transaction_frequency'] = historical_data.groupby('CustomerID')['transaction_diff'].transform(lambda x: (x <= 30).sum())\n",
    "    \n",
    "    successful_outcomes = historical_data[historical_data['outcome'] == 1].groupby('CustomerID')['outcome'].transform('count')\n",
    "    historical_data['successful_outcomes'] = historical_data['CustomerID'].map(successful_outcomes).fillna(0)\n",
    "    historical_data['click_through_rate'] = historical_data['successful_outcomes'] / historical_data['number_of_campaigns']\n",
    "    transaction_counts = historical_data.groupby('CustomerID')['TransactionID'].transform('count')\n",
    "    historical_data['conversion_rate'] = transaction_counts / historical_data['number_of_campaigns']\n",
    "    \n",
    "    w1, w2, w3 = 0.3, 0.5, 0.2\n",
    "    historical_data['engagement_score'] = (w1 * historical_data['conversion_rate'] +\n",
    "                                           w2 * historical_data['click_through_rate'] +\n",
    "                                           w3 * historical_data['transaction_frequency'])\n",
    "    \n",
    "    historical_data = historical_data.drop(columns=['transaction_diff'])\n",
    "    \n",
    "\n",
    "    return historical_data.iloc[-1]\n",
    "\n",
    "# need to be adjusted based on segmentation system application\n",
    "# too long, I really cannot handle it\n",
    "def update_segmentation(row):\n",
    "    if row['CustAge'] > 40 and row['CustAccountBalance'] > 100000:\n",
    "        return 'high_value_segment'\n",
    "    elif row['CustAge'] <= 40 and row['CustAccountBalance'] <= 100000:\n",
    "        return 'low_value_segment'\n",
    "    else:\n",
    "        return 'mid_value_segment'\n",
    "\n",
    "# need to be adjusted based on recommended system\n",
    "def recommend_action_space(segmentation_label):\n",
    "    if segmentation_label == 'high_value_segment':\n",
    "        return ['premium_offer', 'special_discount']\n",
    "    elif segmentation_label == 'low_value_segment':\n",
    "        return ['standard_offer', 'basic_discount']\n",
    "    else:\n",
    "        return ['standard_offer', 'no_offer']\n",
    "    \n",
    "def multi_armed_bandit(action_space, engagement_score):\n",
    "    num_actions = len(action_space)\n",
    "    preferences = np.zeros(num_actions)\n",
    "    probabilities = np.exp(preferences) / np.sum(np.exp(preferences))\n",
    "    alpha = 0.1  \n",
    "    baseline = 0 \n",
    "\n",
    "    action = np.random.choice(action_space, p=probabilities)\n",
    "    action_index = action_space.index(action)\n",
    "    \n",
    "    reward = engagement_score\n",
    "    baseline = baseline + 0.1 * (reward - baseline)\n",
    "    \n",
    "    for i in range(num_actions):\n",
    "        if i == action_index:\n",
    "            preferences[i] += alpha * (reward - baseline) * (1 - probabilities[i])\n",
    "        else:\n",
    "            preferences[i] -= alpha * (reward - baseline) * probabilities[i]\n",
    "    \n",
    "    probabilities = np.exp(preferences) / np.sum(np.exp(preferences))\n",
    "    \n",
    "    return action, probabilities\n",
    "\n",
    "\n",
    "def process_and_recommend(new_transaction):\n",
    "    processed_data = process_transaction_data(new_transaction)\n",
    "    processed_data['segmentation_label'] = update_segmentation(processed_data)\n",
    "    action_space = recommend_action_space(processed_data['segmentation_label'])\n",
    "    \n",
    "    selected_action, action_probabilities = multi_armed_bandit(action_space, processed_data['engagement_score'])\n",
    "    \n",
    "    print(f\"Selected Action: {selected_action}, Probabilities: {action_probabilities}\")\n",
    "\n",
    "# sample input new data\n",
    "new_transaction = pd.DataFrame({\n",
    "    'TransactionID': ['T1048568'],\n",
    "    'CustomerID': ['C1234567'],\n",
    "    'TransactionDate': ['2023-10-20'],\n",
    "    'TransactionAmount (INR)': [5000],\n",
    "    'outcome': [1],\n",
    "    'number_of_campaigns': [3],\n",
    "    'CustAge': [45],\n",
    "    'CustAccountBalance': [150000]\n",
    "})\n",
    "\n",
    "process_and_recommend(new_transaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible consideration:\n",
    "- maybe we can use change contact-timing(day of the week?) as an action, but I'm not sure if this would be valuable\n",
    "- the structure is easy, given that we already have day and month of last campaign, we can assign distribution of campaign succuess and failure based on the day of the week of the last campaign\n",
    "- this follows similar structure with what I have done for adding campaign number to the transaction data, and will be easily implemented if you desired, no worry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
